{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "241fcc7d",
   "metadata": {},
   "source": [
    "# Cyckle Developer Handbook: Basics\n",
    "This short handbook for developers interested in modifying and contributing to Cyckle-ai provides a fundamental understanding of the source files in Cyckle.\n",
    "## Chapter 1: ```main.pyx```\n",
    "This section will go over the concepts described in the ```main.pyx``` source file.\n",
    "### Generation\n",
    "This code snippet is how Cyckle generates responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026a4673",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All\n",
    "\n",
    "system_prompt = '''You are Cyckle, a helpful AI assistant. Your responses should be clear, direct, and relevant to the user's questions. Aim to be informative yet concise.'''\n",
    "usermodel = GPT4All(\"Phi-3-mini-4k-instruct.Q4_0.gguf\", n_threads=4, allow_download=True)\n",
    "userinput = \"What is the capital of France?\"\n",
    "\n",
    "with usermodel.chat_session(system_prompt=system_prompt):\n",
    "    reponse = usermodel.generate(userinput, max_tokens=8, temp=0.3, top_k=25, top_p=0.9, repeat_penalty=1.1, n_batch=8)\n",
    "    print(reponse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565f8cc3",
   "metadata": {},
   "source": [
    "Lets break it down line by line.\n",
    "#### System Prompt\n",
    "The system prompt works by telling the AI what it is and how to do it's job. Pretty elementary right?\n",
    "#### Usermodel\n",
    "This defines performance parameters and the model being used. Lets look deeper.\n",
    "#### The model\n",
    "At the beginning, we define the model with the following:\n",
    "```python\n",
    "    GPT4All(\"Phi-3-mini-4k-instruct.Q4_0.gguf\")\n",
    "```\n",
    "This tells GPT4All that it must use and initialize the ```Phi3-mini``` AI model. If it does not exist, it will search for it on ```gpt4all.io``` in order to download it.\n",
    "#### ```n_threads```\n",
    "In the middle we state the following:\n",
    "```python\n",
    "    n_threads=4\n",
    "```\n",
    "This tells GPT4All that we want to utilize 4 CPU threads (The amount that would be used if you had a dual-core processor) in order to process the AI model.\n",
    "#### Download Parameter\n",
    "Lastly, we state the following:\n",
    "```python\n",
    "    allow_download=true\n",
    "```\n",
    "This tells GPT4All that it can download data, ```.gguf``` files, and prompt templates from the web.\n",
    "#### Generation Parameters\n",
    "The following parameters are used for generating Cyckle's responses\n",
    "```python\n",
    "    max_tokens=8, temp=0.3, top_k=25, top_p=0.9, repeat_penalty=1.1, n_batch=8\n",
    "```\n",
    "Here's an analysis of each parameter:\n",
    "\n",
    "```max_tokens``` The maximum length of response in tokens. (Can be changed without worry)\n",
    "\n",
    "```temp``` The balance between creativity and precision with values between 1 and 0 where 1 is the most creative on the scale and 0 is the most precise on the scale.\n",
    "\n",
    "```top_k``` The scale between 0 and 50 where lower values have more predictable word outputs and higher values are more random word outputs\n",
    "\n",
    "```top_p``` Similar to ```top_k``` but could be defined as a parameter for the words it will use. It has values ranging between 0 and 1 with 1 being more wild and a large pool of words or 0 with a more predictable, smaller word pool.\n",
    "\n",
    "```n_batch``` A lot more technical than the previous parameters, but can be easily described as the amount of tokens being processed at once. Usually for low-spec machines, 4 is a good value but it can be as high as you want until you run out of RAM or VRAM.\n",
    "\n",
    "```repeat_penalty``` Penalizes the AI for repeating. Typical values are between 1.1 and 1.3 because any higher and the AI will miss out on important repeats.\n",
    "\n",
    "### Optimization\n",
    "\n",
    "This code snippet demonstrates how Cyckle optimizes for the user's CPU/GPU. (Adapted for Python instead of Cython)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd1e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "def optimize():\n",
    "    physcores = psutil.cpu_count(logical=False)\n",
    "    logicores = psutil.cpu_count(logical=True)\n",
    "    return physcores, logicores\n",
    "\n",
    "physcores, logicores = optimize()\n",
    "threads = min(logicores, 8)\n",
    "\n",
    "GPT4All(\"Phi-3-mini-4k-instruct.Q4_0.gguf\", n_threads=threads)\n",
    "print(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e452b56",
   "metadata": {},
   "source": [
    "Lets break this down line by line.\n",
    "\n",
    "#### ```psutil```\n",
    "```psutil``` is utilized for detecting certain characteristics of a user's system. In this case, it detects the threads in a user's processor.\n",
    "\n",
    "#### ```physcores```\n",
    "The amount of physical cores detected by ```psutil```.\n",
    "\n",
    "#### ```logicores```\n",
    "The amount of logical cores detected by ```psutil```.\n",
    "\n",
    "### Model list\n",
    "The following dictionary demonstrates the multi-model system in Cyckle. (Adapted for Python from Cython)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daa71b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All\n",
    "\n",
    "mini_models = {\n",
    "    \"Llama-3.2-1B\": {\n",
    "        \"model\": \"Llama-3.2-1B-Instruct-Q4_0.gguf\",\n",
    "        \"id\": \"llama3\",\n",
    "        \"purpose\": \"Ultra-light model for creative tasks.\"\n",
    "    },\n",
    "    \"Phi3-mini\": {\n",
    "        \"model\": \"Phi-3-mini-4k-instruct.Q4_0.gguf\",\n",
    "        \"id\": \"phi3\",\n",
    "        \"purpose\": \"Lightweight model for logical and reasoning tasks.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "model = mini_models[\"Phi3-mini\"][\"model\"]\n",
    "model_id = mini_models[\"Phi3-mini\"][\"id\"]\n",
    "model_purpose = mini_models[\"Phi3-mini\"][\"purpose\"]\n",
    "print(f\"Model: {model}, ID: {model_id}, Purpose: {model_purpose}\")\n",
    "\n",
    "lmodel = mini_models[\"Llama-3.2-1B\"][\"model\"]\n",
    "lmodel_id = mini_models[\"Llama-3.2-1B\"][\"id\"]\n",
    "lmodel_purpose = mini_models[\"Llama-3.2-1B\"][\"purpose\"]\n",
    "print(f\"Model: {lmodel}, ID: {lmodel_id}, Purpose: {lmodel_purpose}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd017c2",
   "metadata": {},
   "source": [
    "Lets look at each property bit by bit.\n",
    "\n",
    "#### ```model```\n",
    "This is the identifier that ```gpt4all``` needs to figure out what model we are utilizing.\n",
    "\n",
    "#### ```id```\n",
    "This serves as a basic identifier for when switching models with the ```modelconfig``` option.\n",
    "\n",
    "#### ```purpose```\n",
    "This is currently an unused parameter that describes the use and well... PURPOSE of the model.\n",
    "\n",
    "## Chapter 2: ```data.json```\n",
    "This section will go over the system used to save the ```modtokens``` parameter to JSON.\n",
    "\n",
    "### Reading Tokens\n",
    "This snippet demonstrates the function of reading tokens from the JSON file. (Adapted to Python from Cython.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af960acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "filename = \"data.json\"\n",
    "\n",
    "def read_tokens_from_json():\n",
    "    try:\n",
    "        with open(filename, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            return data.get(\"tokens\", 96)\n",
    "    except FileNotFoundError:\n",
    "        print(\"data.json not found. Falling back to default 256.\")\n",
    "        return 256\n",
    "\n",
    "modtokens = read_tokens_from_json()\n",
    "\n",
    "datadict = {\n",
    "    \"tokens\": modtokens\n",
    "}\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(datadict, f, indent=4)\n",
    "    print(\"Data written to:\", filename)\n",
    "\n",
    "print(\"Token amount:\", modtokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc762c07",
   "metadata": {},
   "source": [
    "Want to experiment with this? Modify the ```data.json``` file in the root of this directory!\n",
    "\n",
    "Anyways, let's see what is going on here!\n",
    "\n",
    "#### Reading the tokens\n",
    "The following section of our snippet defines how Cyckle reads tokens from ```data.json```.\n",
    "```python\n",
    "    def read_tokens_from_json():\n",
    "    try:\n",
    "        with open(filename, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            return data.get(\"tokens\", 96)\n",
    "    except FileNotFoundError:\n",
    "        print(\"data.json not found. Falling back to default 256.\")\n",
    "        return 256\n",
    "```\n",
    "This tells Cyckle to load from ```data.json``` expecting the following data structure:\n",
    "```JSON\n",
    "{\n",
    "    \"tokens\": modtokens\n",
    "}\n",
    "```\n",
    "***Note: If the function fails to find the relevant information or the file ```data.json```, it will fallback to the default number of 256 tokens.***\n",
    "\n",
    "#### Writing the tokens\n",
    "The following section of our snippet defines how Cyckle writes tokens to ```data.json```.\n",
    "```python\n",
    "    with open(filename, 'w') as f:\n",
    "    json.dump(datadict, f, indent=4)\n",
    "    print(\"Data written to:\", filename)\n",
    "```\n",
    "This tells Cyckle that the ```datadict``` dictionary (which is formatted to the liking of JSON) should be written into ```data.json```. It also provides extra formatting with the ```indent=4``` option which tells Cyckle to save the JSON with 4 spaces per indentation.\n",
    "\n",
    "## Chapter 3: ```setup.py```\n",
    "This section delves into the technical aspects of the ```setup.py``` file used to convert ```.pyx``` into ```.c``` and ```.h``` source files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f74f82",
   "metadata": {},
   "source": [
    "```python\n",
    "from setuptools import setup, Extension\n",
    "from Cython.Build import cythonize\n",
    "\n",
    "extensions = [\n",
    "    Extension(\n",
    "        name=\"main\",\n",
    "        sources=[\"main.pyx\"],\n",
    "        extra_compile_args=[\"-fPIC\"],\n",
    "    )\n",
    "]\n",
    "\n",
    "setup(\n",
    "    name=\"CyckleAI\",\n",
    "    ext_modules=cythonize(extensions, language_level=\"3\"),\n",
    "    zip_safe=False,\n",
    ")\n",
    "```\n",
    "\n",
    "Right now this may seem like technical nonsense, but let's explain and analyze this.\n",
    "\n",
    "#### Extension Compiler Args\n",
    "Cython (for some unholy reason) considers this standalone app, an extension. Inside of this, we have a compiler argument titled ```-fPIC```. This argument tells Cython to build this with shared libraries, hence the reason it creates ```main.c``` and ```main.h``` from ```main.pyx```.\n",
    "\n",
    "#### Language Level\n",
    "The ```language_level``` parameter defines whether or not you are using Python 2 or 3. Of course, Cyckle isn't ancient and thus does not use Python 3, henceforth ```language_level``` is set to 3.\n",
    "\n",
    "#### Zip Safe\n",
    "The ```zip_safe``` parameter defines whether your application should be executed as a \"ZipApp\" (Basically a normal executable, but in a portable ```.zip```, ```.tar```, etc. format). This is set to false as ZipApps are unsuitable for Cyckle's use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
